#!/usr/bin/env python

"""
Runs MPAS-Analysis via a configuration file (e.g. `config.analysis`)
specifying analysis options.

Authors
-------
Xylar Asay-Davis, Phillip J. Wolfram
"""

import matplotlib as mpl
import argparse
import traceback
import sys
import pkg_resources
import shutil
import os

from mpas_analysis.configuration import MpasAnalysisConfigParser

from mpas_analysis.shared.io.utility import build_config_full_path, \
    make_directories

from mpas_analysis.shared.html import generate_html

from mpas_analysis.shared import AnalysisTask


def update_generate(config, generate):  # {{{
    """
    Update the 'generate' config option using a string from the command line.

    Parameters
    ----------
    config : ``MpasAnalysisConfigParser`` object
        contains config options

    generate : str
        a comma-separated string of generate flags: either names of analysis
        tasks or commands of the form ``all_<tag>`` or ``no_<tag>`` indicating
        that analysis with a given tag should be included or excluded).

    Authors
    -------
    Xylar Asay-Davis
    """

    # overwrite the 'generate' in config with a string that parses to
    # a list of string
    generateList = generate.split(',')
    generateString = ', '.join(["'{}'".format(element)
                                for element in generateList])
    generateString = '[{}]'.format(generateString)
    config.set('output', 'generate', generateString)  # }}}


def run_parallel_tasks(config, analyses, taskCount):
    # {{{
    """
    Launch new processes for parallel tasks, allowing up to ``taskCount``
    tasks to run at once.

    Parameters
    ----------
    config : ``MpasAnalysisConfigParser`` object
        contains config options

    analyses : list of ``AnalysisTask`` objects
        A list of analysis tasks to run

    taskCount : int
        The maximum number of tasks that are allowed to run at once

    Authors
    -------
    Xylar Asay-Davis
    """

    taskCount = min(taskCount, len(analyses))

    runningTasks = {}
    for analysisTask in analyses[0:taskCount]:
        print 'Running {}'.format(analysisTask.taskName)
        analysisTask.start()
        runningTasks[analysisTask.taskName] = analysisTask

    remainingTasks = analyses[taskCount:]
    tasksWithErrors = []
    while len(runningTasks.keys()) > 0:
        analysisTask = wait_for_task(runningTasks)
        taskName = analysisTask.taskName
        if analysisTask._runStatus.value == AnalysisTask.SUCCESS:
            print "   Task {} has finished successfully.".format(taskName)
        elif analysisTask._runStatus.value == AnalysisTask.FAIL:
            print "ERROR in task {}.  See log file {} for details".format(
                taskName, analysisTask._logFileName)
            tasksWithErrors.append(taskName)
        else:
            print "Unexpected status from in task {}.  This may be a " \
                  "bug.".format(taskName)
        # remove the process from the process dictionary (no need to bother)
        runningTasks.pop(taskName)

        if len(remainingTasks) > 0:
            analysisTask = remainingTasks[0]
            remainingTasks = remainingTasks[1:]

            print 'Running {}'.format(analysisTask.taskName)
            analysisTask.start()
            runningTasks[analysisTask.taskName] = analysisTask

    # raise the last exception so the process exits with an error
    errorCount = len(tasksWithErrors)
    if errorCount == 1:
        print "There were errors in task {}".format(tasksWithErrors[0])
        sys.exit(1)
    elif errorCount > 0:
        print "There were errors in {} tasks: {}".format(
                errorCount, ', '.join(tasksWithErrors))
        sys.exit(1)
    # }}}


def wait_for_task(runningTasks, timeout=0.1):  # {{{
    """
    Build a list of analysis modules based on the 'generate' config option.
    New tasks should be added here, following the approach used for existing
    analysis tasks.

    Parameters
    ----------
    runningTasks : dict of ``AnalysisTasks``
        The tasks that are currently running, with task names as keys

    Returns
    -------
    analysisTask : ``AnalysisTasks``
        A task that finished

    Authors
    -------
    Xylar Asay-Davis
    """
    # necessary to have a timeout so we can kill the whole thing
    # with a keyboard interrupt
    while True:
        for analysisTask in runningTasks.itervalues():
            analysisTask.join(timeout=timeout)
            if not analysisTask.is_alive():
                return analysisTask  # }}}


def build_analysis_list(config):  # {{{
    """
    Build a list of analysis tasks. New tasks should be added here, following
    the approach used for existing analysis tasks.

    Parameters
    ----------
    config : ``MpasAnalysisConfigParser`` object
        contains config options

    Returns
    -------
    analyses : list of ``AnalysisTask`` objects
        A list of all analysis tasks

    Authors
    -------
    Xylar Asay-Davis
    """

    # choose the right rendering backend, depending on whether we're displaying
    # to the screen
    if not config.getboolean('plot', 'displayToScreen'):
        mpl.use('Agg')

    # analysis can only be imported after the right MPL renderer is selected
    from mpas_analysis import ocean
    from mpas_analysis import sea_ice

    # analyses will be a list of analysis classes
    analyses = []

    # Ocean Analyses

    analyses.append(ocean.ClimatologyMapMLD(config))
    analyses.append(ocean.ClimatologyMapSST(config))
    analyses.append(ocean.ClimatologyMapSSS(config))
    analyses.append(ocean.TimeSeriesOHC(config))
    analyses.append(ocean.TimeSeriesSST(config))
    analyses.append(ocean.MeridionalHeatTransport(config))
    analyses.append(ocean.StreamfunctionMOC(config))
    analyses.append(ocean.IndexNino34(config))

    # Sea Ice Analyses
    analyses.append(sea_ice.ClimatologyMapSeaIceConc(config, hemisphere='NH'))
    analyses.append(sea_ice.ClimatologyMapSeaIceThick(config, hemisphere='NH'))
    analyses.append(sea_ice.ClimatologyMapSeaIceConc(config, hemisphere='SH'))
    analyses.append(sea_ice.ClimatologyMapSeaIceThick(config, hemisphere='SH'))
    analyses.append(sea_ice.TimeSeriesSeaIce(config))

    return analyses  # }}}


def determine_analyses_to_generate(analyses):  # {{{
    """
    Build a list of analysis tasks to run based on the 'generate' config
    option (or command-line flag) and prerequisites and subtasks of each
    requested task.  Each task's ``setup_and_check`` method is called in the
    process.

    Parameters
    ----------
    analyses : list of ``AnalysisTask`` objects
        A list of all analysis tasks

    Returns
    -------
    analysesToGenerate : list of ``AnalysisTask`` objects
        A list of analysis tasks to run

    Authors
    -------
    Xylar Asay-Davis
    """

    # check which analysis we actually want to generate and only keep those
    analysesToGenerate = []
    for analysisTask in analyses:
        # for each anlaysis module, check if we want to generate this task
        # and if the analysis task has a valid configuration
        if analysisTask.check_generate():
            add = False
            try:
                analysisTask.setup_and_check()
                add = True
            except (Exception, BaseException):
                traceback.print_exc(file=sys.stdout)
                print "ERROR: analysis module {} failed during check and " \
                    "will not be run".format(analysisTask.taskName)
            if add:
                analysesToGenerate.append(analysisTask)

    return analysesToGenerate  # }}}


def run_analysis(config, analyses):  # {{{
    """
    Run one or more analysis tasks

    Parameters
    ----------
    config : ``MpasAnalysisConfigParser`` object
        contains config options

    analyses : list of ``AnalysisTask`` objects
        A list of analysis tasks to run

    Raises
    ------
    Exception:
        If one or more tasks raise exceptions, re-raises the last exception
        after all tasks have completed to indicate that there was a problem

    Authors
    -------
    Xylar Asay-Davis
    """

    # run each analysis task
    tasksWithErrors = []
    lastStacktrace = None
    for analysisTask in analyses:
        analysisTask.run(writeLogFile=False)
        if analysisTask._runStatus.value == AnalysisTask.FAIL:
            lastStacktrace = analysisTask._stackTrace
            tasksWithErrors.append(analysisTask.taskName)

    if config.getboolean('plot', 'displayToScreen'):
        import matplotlib.pyplot as plt
        plt.show()

    # See if there were errors; exit(1) if so
    errorCount = len(tasksWithErrors)
    if errorCount == 1:
        if len(analyses) > 1:
            print "There were errors in task {}".format(tasksWithErrors[0])
            print "The stacktrace was:"
            print lastStacktrace
        sys.exit(1)
    elif errorCount > 0:
        print "There were errors in {} tasks: {}".format(
                errorCount, ', '.join(tasksWithErrors))
        print "The last stacktrace was:"
        print lastStacktrace
        sys.exit(1)

    # }}}


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("--setup_only", dest="setup_only", action='store_true',
                        help="If only the setup phase, not the run or HTML "
                        "generation phases, should be executed.")
    parser.add_argument("--html_only", dest="html_only", action='store_true',
                        help="If only the setup and HTML generation phases, "
                        "not the run phase, should be executed.")
    parser.add_argument("-g", "--generate", dest="generate",
                        help="A list of analysis modules to generate "
                        "(nearly identical generate option in config file).",
                        metavar="ANALYSIS1[,ANALYSIS2,ANALYSIS3,...]")
    parser.add_argument("-l", "--list", dest="list", action='store_true',
                        help="List the available analysis tasks")
    parser.add_argument("-p", "--purge", dest="purge", action='store_true',
                        help="Purge the analysis by deleting the output"
                        "directory before running")
    parser.add_argument('configFiles', metavar='CONFIG',
                        type=str, nargs='*', help='config file')
    args = parser.parse_args()

    # add config.default to cover default not included in the config files
    # provided on the command line
    if pkg_resources.resource_exists('mpas_analysis', 'config.default'):
        defaultConfig = pkg_resources.resource_filename('mpas_analysis',
                                                        'config.default')
        configFiles = [defaultConfig] + args.configFiles
    else:
        print 'WARNING: Did not find config.default.  Assuming other config ' \
              'file(s) contain a\n' \
              'full set of configuration options.'
        configFiles = args.configFiles

    config = MpasAnalysisConfigParser()
    config.read(configFiles)

    if args.list:
        analyses = build_analysis_list(config)
        for analysisTask in analyses:
            print '{}\n    tags: {}'.format(analysisTask.taskName,
                                            ', '.join(analysisTask.tags))
        sys.exit(0)

    if args.purge:
        outputDirectory = config.get('output', 'baseDirectory')
        if not os.path.exists(outputDirectory):
            print 'Output directory {} does not exist.\n' \
                  'No purge necessary.'.format(outputDirectory)
        else:
            print 'Deleting contents of {}'.format(outputDirectory)
            shutil.rmtree(outputDirectory)

    if args.generate:
        update_generate(config, args.generate)

    logsDirectory = build_config_full_path(config, 'output',
                                           'logsSubdirectory')
    make_directories(logsDirectory)
    make_directories('{}/configs/'.format(logsDirectory))

    analyses = build_analysis_list(config)
    analyses = determine_analyses_to_generate(analyses)

    parallelTaskCount = config.getWithDefault('execute', 'parallelTaskCount',
                                              default=1)

    if not args.setup_only and not args.html_only:
        if parallelTaskCount <= 1 or len(analyses) == 1:
            run_analysis(config, analyses)
        else:
            run_parallel_tasks(config, analyses, parallelTaskCount)

    if not args.setup_only:
        generate_html(config, analyses)

# vim: foldmethod=marker ai ts=4 sts=4 et sw=4 ft=python
